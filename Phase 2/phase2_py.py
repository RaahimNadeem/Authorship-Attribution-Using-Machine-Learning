# -*- coding: utf-8 -*-
"""phase2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ozxPJROiwl0q5xbuiB18PMYXlK55fTha
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -U sentence-transformers                         #installing the sentence-transformers module on Colab (remove for VS Code)

#importing relevant libraries
import pandas as pd
import numpy as np
from string import punctuation
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_predict, RandomizedSearchCV,cross_validate
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import BaggingClassifier, VotingClassifier, AdaBoostClassifier, RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import validation_curve
from sklearn.model_selection import GridSearchCV

"""# Reading Data"""

df = pd.DataFrame(columns=["tweet", "label"])
df.head()



labels = ["bill gates", "downing street", "mia mottley", "roger federer", "white house"]                                                    #creating list of all possible output labels
files = ["BillGates_task1.csv", "downingstreet_task1.csv", "miaamormottley_task1.csv", "rogerfederer_task1.csv", "WhiteHouse_task1.csv"]    #creating list of all members' .csv files

for file, label in zip(files, labels):                                                                                                      #iterating through files and output labels (each file corresponds to a different output label)
    tweets_df = pd.read_csv(f"/content/drive/My Drive/ML/project/{file}")                                                                                                           #reading file with pandas
    tweets_df = tweets_df.rename(columns={"Tweet": "tweet", "Tweets": "tweet", "tweets": "tweet"})                                          #renaming dataframe column names where required for uniformity
    tweets_df["label"] = label                                                                                                              #storing appropriate label with this data

    df = pd.concat([tweets_df.iloc[:, [-2, -1]], df], )                                                                                     #concatenating this iteration's data into one big dataframe

df.head()

print("The dimensions of our cumulative dataframe are:", df.shape)                                                                          #printing dimensions of our final dataframe

print("Counts:")                                                                                                                            #printing the number of records per label (we scraped 1000 tweets each)
for label in df["label"].unique():                                                                                                          #iterating through all the unique labels in our dataframe 
    print(f"{label}: {sum(df['label'] == label)}")                                                                                          #printing number of entries with this label

"""# Data Cleaning"""

def clean_data(series):
    stop_words = ["i", "i'm", "me", "my", "myself", "we", "our", "ours", "ourselves",
              "you", "you're", "you've", "you'll", "you'd", "your", "yours", "yourself",
              "yourselves", "he", "him", "his", "himself", "she", "she's", "her", "hers",
              "herself", "it", "it's", "its", "itself", "they", "them", "their", "theirs",
              "themselves", "what", "which", "who", "whom", "this", "that", "that'll",
              "these", "those", "am", "is", "are", "was", "were", "be", "been", "being",
              "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an",
              "the", "and", "but", "if", "or", "because", "as", "until", "while", "of",
              "at", "by", "for", "with", "about", "against", "between", "into", "through",
              "during", "before", "after", "above", "below", "to", "from", "up",
              "down", "in", "out", "on", "off", "over", "under", "again", "further", "then",
              "once", "here", "there", "when", "where", "why", "how", "all", "any", "both",
              "each", "few", "more", "most", "other", "some", "such", "no", "nor", "not",
              "only", "own", "same", "so", "than", "too", "very", "s", "t", "can", "will",
              "just", "don", "don't", "should", "should've", "now", "d", "ll", "m", "o",
              "re", "ve", "y", "ain", "aren", "aren't", "couldn", "couldn't", "didn",
              "didn't", "doesn", "doesn't", "hadn", "hadn't", "hasn", "hasn't", "haven",
              "haven't", "isn", "isn't", "ma", "mightn", "mightn't", "mustn", "mustn't",
              "needn", "needn't", "shan", "shan't", "shouldn", "shouldn't", "wasn",
              "wasn't", "weren", "weren't", "won", "won't", "wouldn", "wouldn't", "b", "c",
              "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s",
              "t", "u", "v", "w", "x", "y", "z"]
    
    # tags_accounts = "(@|#)\w*"
    twitter_images = "https://t.co/[a-zA-Z0-9]{10}"
    non_ascii = "[^\x00-\x7F]+"
    punctuations = f"[{punctuation}]"
    numbers = "\d+"
    
    series = series.str.lower()                                             #convering to lower case
    series = series.str.split().str.join(" ")                               
    series = series.str.replace(twitter_images, " ", regex=True)            #removing twitter images
    series = series.str.replace(numbers, " ", regex=True)                   #removing numbers
    series = series.str.replace(punctuations, " ", regex=True)              #removing punctuations
    series = series.apply(lambda tweet: " ".join([word for word in tweet.split() if word not in stop_words]))     #removing stop words

    return series

df["tweet"] = clean_data(df["tweet"])
df = df[df["tweet"].str.len() > 0]

print("Counts:")                                                                                                                            #printing the number of records per label (we scraped 1000 tweets each)
for label in df["label"].unique():                                                                                                          #iterating through all the unique labels in our dataframe 
    print(f"{label}: {sum(df['label'] == label)}")                                                                                          #printing number of entries with this label

"""# Shuffle Data"""

df = df.sample(frac=1, random_state=0)

df.head(10)

df.shape

"""# Train Test Split"""

train_df, test_df = train_test_split(df, train_size=0.8, random_state=0)

print("Train:", train_df.shape)
print("Test:", test_df.shape)

"""# Feature Extraction"""

# getting bag of words from the complete data set
def get_bow(dataset):
    return list(set([word for sentence in dataset for word in sentence.split()]))

# getting bag of words feature vector for each datapoint
def get_features(dataset, bow):
    word_to_idx = dict(map(lambda pair: (pair[1], pair[0]), enumerate(bow)))
    features = np.zeros((len(dataset), len(bow)), dtype=int)
    
    for idx, sentence in enumerate(dataset):
        for word in sentence.split():
            word_idx = word_to_idx.get(word)
            if word_idx:
                
                features[idx][word_idx] += 1
            
    return features

"""## BOW"""

bow = get_bow(train_df["tweet"])
bow[:10]

len(bow)

x_train_bow = get_features(train_df["tweet"], bow)
x_train_bow[:10]

y_train_bow = train_df["label"].to_numpy()
y_train_bow[:10]

x_test_bow = get_features(test_df["tweet"], bow)
x_test_bow[:10, :10]

y_test_bow = test_df["label"].to_numpy()
y_test_bow[:10]

"""## Embeddings"""

model = SentenceTransformer('all-MiniLM-L6-v2')

x_train_emb = np.apply_along_axis(model.encode, 0, train_df["tweet"].to_numpy())
x_train_emb[:10, :10]

y_train_emb = train_df["label"].to_numpy()
y_train_emb[:10]

x_test_emb = np.apply_along_axis(model.encode, 0, test_df["tweet"].to_numpy())
x_test_emb[:10, :10]

y_test_emb = test_df["label"].to_numpy()
y_test_emb[:10]

"""# Modelling

Evaluation Function
"""

def evaluate(y_true, y_pred):
    cf_mat = confusion_matrix(y_true, y_pred)
    report = classification_report(y_true, y_pred,output_dict=True)
    # "{:.2f}".format(report['macro avg']['f1-score'])
    print("Accuracy:  ","{:.2f}".format(report['accuracy']*100))
    print("Precision: ", "{:.2f}".format(report['macro avg']['precision']*100))  
    print("Recall:    ","{:.2f}".format(report['macro avg']['recall']*100))
    print("F1-Score:  ","{:.2f}".format(report['macro avg']['f1-score']*100))           #printing classification report
    
    labels = ['bill gates', 'downing street', 'mia mottley', 'roger federer', 'white house']
    
    plt.figure(figsize=(10, 8))
    ax = plt.subplot()
    sns.heatmap(cf_mat, annot=True, fmt="g", ax=ax, cmap='mako_r')      #printing confusion matrix
    ax.set_xlabel('Predicted labels')
    ax.set_ylabel('True labels')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels(labels)
    ax.yaxis.set_ticklabels(labels)
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.show()

"""Cross Validation Function"""

def cross_validate_plot(model, x, y, folds=5):
    metrics = cross_validate(model, x, y, return_train_score=True)        #runing cross validation

    metric_df = pd.DataFrame({"training": metrics["train_score"], "validation": metrics["test_score"], "fold": list(range(1, folds+1))})
    metric_df = pd.melt(metric_df, id_vars=["fold"], value_vars=["training", "validation"], var_name="type", value_name="accuracy")

    plt.figure(figsize=(10, 8))
    sns.lineplot(data=metric_df, x="fold", y="accuracy", hue="type")        
    plt.xlabel("folds")
    plt.show()

"""# KNN

## BOW

Tuning
"""

# using GridSearhCV to tune hyperparameters for KNN using Bag of word feature set
params = {
    "n_neighbors": list(range(1, 10)), 
    "weights": ["uniform", "distance"],         #tuning number of neighbors, weights and p
    "p": [1, 2]
}

knn = KNeighborsClassifier()

clf = GridSearchCV(knn, params)
search = clf.fit(x_train_bow[:500], y_train_bow[:500])
search.best_params_

"""Cross Validation

"""

knn=KNeighborsClassifier(n_neighbors=3,p=2,weights='distance')
cross_validate_plot(knn,x_train_bow,y_train_bow)

"""Running final Model"""

# runing model with previously found best parameters
model = KNeighborsClassifier(n_neighbors=3, weights="distance", p=2)
model.fit(x_train_bow, y_train_bow)

y_pred = model.predict(x_test_bow)
evaluate(y_test_bow, y_pred)

"""## Embeddings

Tuning
"""

# using GridSearhCV to tune hyperparameters for KNN using Embedding feature set
params = {                                        
    "n_neighbors": list(range(1, 10)),                        #tuning number of neighbors, weights and p
    "weights": ["uniform", "distance"],
    "p": [1, 2]
}

knn = KNeighborsClassifier()

clf = GridSearchCV(knn, params)
search = clf.fit(x_train_emb[:500], y_train_emb[:500])
search.best_params_

"""Cross Validation"""

knn=KNeighborsClassifier(weights="distance",p=1,n_neighbors=4)
cross_validate_plot(knn,x_train_emb,y_train_emb)

"""Running Final Model"""

# runing model with previously found best parameters
model = KNeighborsClassifier(n_neighbors=4, weights='distance', p=1)
model.fit(x_train_emb, y_train_emb)

y_pred = model.predict(x_test_emb)
evaluate(y_test_emb, y_pred)

"""## NNs

## BOW

Tuning
"""

# using GridSearhCV to tune hyperparameters for NN using Bag Of Words feature set

params = {                                                          #tuning activation function
    "activation": ['identity','logistic','tanh','relu']
    
}



nn=MLPClassifier(hidden_layer_sizes=(256, 256, 128, 64))

clf = GridSearchCV(nn, params)
search = clf.fit(x_train_bow[:500], y_train_bow[:500])
search.best_params_

"""Cross Validation"""

nn=MLPClassifier(hidden_layer_sizes=(256, 256, 128, 64),activation='tanh')
cross_validate_plot(nn,x_train_bow,y_train_bow)

"""Runing Final Model"""

# runing model with previously found best parameters
model = MLPClassifier(hidden_layer_sizes=(256, 256, 128, 64),activation='tanh')
model.fit(x_train_bow, y_train_bow)

y_pred = model.predict(x_test_bow)
evaluate(y_test_bow, y_pred)

"""# Embeddings

Tuning
"""

# using GridSearhCV to tune hyperparameters for NN using Embedding feature set

params = {                                                        #tuning activation function
    "activation": ['identity','logistic','tanh','relu']
    
}



nn=MLPClassifier(hidden_layer_sizes=(256, 256, 128, 64))

clf = GridSearchCV(nn, params)
search = clf.fit(x_train_emb[:500], y_train_emb[:500])
search.best_params_

"""Cross Validation"""

nn=MLPClassifier(hidden_layer_sizes=(256, 256, 128, 64),activation='relu')
cross_validate_plot(nn,x_train_emb,y_train_emb)

"""Runing Final Model"""

# runing model with previously found best parameters
model = MLPClassifier(hidden_layer_sizes=(256, 128, 128),activation='relu')
model.fit(x_train_emb, y_train_emb)

y_pred = model.predict(x_test_emb)
evaluate(y_test_emb, y_pred)

"""## Ensemble Methods

## BOW
"""

# cross validation for finding best emsemble method using bag of words feature set
labels = ["Bagging", "Ada Boost", "Random Forest"]
metrics = []
methods = [BaggingClassifier, AdaBoostClassifier, RandomForestClassifier]

for method in methods:
    y_pred = cross_val_predict(method(), x_train_bow, y_train_bow)
    metrics.append(accuracy_score(y_train_bow, y_pred))

plt.figure(figsize=(10, 8))
sns.barplot(x=labels, y=metrics)
plt.xlabel("Methods")
plt.ylabel("Accuracy")
plt.show()

"""## Tuning Random Forest"""

# Random forest chosen as best emsemle method for bag of word feature set
# finding best parameters for random forest using bag of words feature set
params = {
    "n_estimators": np.linspace(100, 200, 11, dtype=int), 
    "max_depth": np.linspace(10, 120, 12, dtype=int),
    "max_features": ["sqrt", "log2", None]
}

model = RandomForestClassifier(random_state = 0)

clf = GridSearchCV(model, params)
search = clf.fit(x_train_bow[:500], y_train_bow[:500])
search.best_params_

# runing model with previously found best parameters
model = RandomForestClassifier(n_estimators=180, max_depth=80, max_features="log2")
model.fit(x_train_bow, y_train_bow)

y_pred = model.predict(x_test_bow)
evaluate(y_test_bow, y_pred)

"""## Embeddings"""

# cross validation for finding best emsemble method using embedding feature set
labels = ["Bagging", "Ada Boost", "Random Forest"]
metrics = []
methods = [BaggingClassifier, AdaBoostClassifier, RandomForestClassifier]

for method in methods:
    y_pred = cross_val_predict(method(), x_train_emb, y_train_emb)
    metrics.append(accuracy_score(y_train_emb, y_pred))

plt.figure(figsize=(10, 8))
sns.barplot(x=labels, y=metrics)
plt.xlabel("Methods")
plt.ylabel("Accuracy")
plt.show()

"""## Tuning Random Forest"""

params = {
    "n_estimators": np.linspace(200, 350, 11, dtype=int), 
    "max_depth": np.linspace(60, 160, 12, dtype=int),
    "max_features": ["sqrt", "log2", None]
}

model = RandomForestClassifier(random_state = 0)

clf = RandomizedSearchCV(model, params)
search = clf.fit(x_train_emb[:500], y_train_emb[:500])
search.best_params_

model = RandomForestClassifier(n_estimators=350, max_depth=132, max_features="log2")
model.fit(x_train_emb, y_train_emb)

y_pred = model.predict(x_test_emb)
evaluate(y_test_emb, y_pred)

"""**Task 4**

Q1: Which model performed best and why do you think that is? 

A1: In our tests, the Random Forest (RF) ensemble model with embeddings implemented produced the best performance for the authorship attribution problem, achieving an accuracy of 91% on our test set. We believe this is due to the resilience of the RF model to the issue of noise and outliers in our feature set.

Embedding the tweet data (i.e., producing a low-dimensional numerical representation of the tweets) with the “all-MiniLM-L6-v2” transformer model pairs nicely with the RF approach, since any noise that was left behind after embedding will be handled by the model. 

Furthermore, the RF model is resistant to overfitting because it averages the results of each tree at each iteration (reducing the overall variance while keeping the bias the same). 

The NN and kNN models, on the other hand, are not as resistant to overfitting (which is an issue due to the heavily contextual nature of the tweets and the overlap between our political account data (i.e., White House, Downing Street, etc.)).

Q2: Which features gave better results for each model? 

A2: In the case of our kNN model, the embedded features gave better results (a whopping 92% versus the 65% produced using the bag of words features). This is hardly surprising, seeing as the bag of words features are very sparse in nature. This makes it very difficult for the kNN model to perform well on them, since such models suffer from the curse of dimensionality (i.e., the issue that arises as a result of there being no "near" features). Hence, the kNN model performs many orders of magnitude better on the more dense embedded data.

In the case of our ensemble Random Forest (RF) model, the embeddings also gave much better results (and here, it was at least comparable with the bag of words features producing an accuracy of 90% vs. the 91% accuracy recorded using the embedded data). Using the tranformer model to embed our data effectively stores the *context* of each tweet in the feature set while also reducing the dimensionality of the data. This is, of course, vital for a use case as context-dependent as authorship attribution using scraped tweets, and this was reflected in the high accuracy that our RF model reported.

In the case of the NN model, however, we did observe that the embedded features performed to a lower degree of accuracy than the bag of words feature set. One possible explanation for this is that since the MLPClassifier has a number of parameters with many ranges in between, we could not run GridSearhCV or RandomizedSearchCV exhaustively (simply due to a shortage of time). 

Hence it is possible that we did not achieve the highest possible accuracy the Neural Network Model using embeddings. The current accuracy of about 90% for embedded data and 93% for the bag of words feature set is still considerable, however. To our surprise, the NN model was able to tackle the sparse bag of words feature set quite well. 

Q3: What effect would increasing the classes to 150 have?

A3: Adding more classes to the data can make the model more complex and can require more data to train the model accurately. This can make the training process more time-consuming and can also increase the risk of overfitting, where the model performs well on the training data but poorly on unseen data.

When working with a k-nearest neighbors (k-NN) predictive model, increasing the number of classes in the data would typically increase the complexity of the model, since the number of classes would require further computations.

In the case of the random forest,  increasing the number of classes in the data would not necessarily affect the performance of the random forest model, since the model is designed to handle multi-class classification problems. However, it is worth noting that increasing the number of classes would likely increase the complexity of the model, and might require more trees in the forest to achieve good performance. Additionally, the larger number of classes could potentially make it more challenging to interpret the predictions made by the model.

Finally, in the case of the NN model,  increasing the number of classes in the data would also typically increase the complexity of the model. This is because  the output layer of an NN model typically has one neuron for each class, and the neurons use a softmax activation function to produce a probability distribution over the classes. Increasing the number of classes to 150 would greatly increase the computation required. 

Q4: Suggest improvements to text preparation, feature extraction, and models that can be made
to perform this task better.

A4: Given more time, we would have liked to run the gridsearch cross validation function for our Neural Network model to exhaustively find the most suitable hyperparameters. 

We could also use more advanced techniques to tokenize the text and extract useful features, such as n-grams, and part-of-speech tags. These features can capture important information about the context and meaning of the words, possibly to a greater extent than our current embedded data does. 

We can also remove certain words that overlap across classes (since three of our classes have political contexts and hence would interfere with each other's classification (i.e., the White House, Downing Street, and Mia Mottley)). Ultimately, our goal is to carefully analyze the data and experiment with different approaches to find the best solution.

Q5: What - in your understanding - are the applications of authorship attribution?

A5:  In our understanding, authorship attribution is the process of identifying the author of a given text. 

This can be useful in a variety of contexts, such as in forensic linguistics to identify the author of an anonymous or pseudonymous document, or in the study of literature to attribute texts to specific authors. 

It can also be used to detect plagiarism by comparing the writing style of a given text to that of known authors. In general, authorship attribution can provide valuable insights into the relationship between language and identity.
"""