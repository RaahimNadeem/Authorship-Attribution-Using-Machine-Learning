# -*- coding: utf-8 -*-
"""MuhammadRaahimNadeem_ProjectPhase1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GvWHn_WWdmgsINmENXq3nlrLQjMvOMv-

# ***Header Files***
"""

import pandas as pd
# import snscrape.modules.twitter as sntwitter
import re
import seaborn as sns
import html
import string
import numpy as np

"""# TASK 1"""

tweets_list = []

for i, tweet in enumerate(sntwitter.TwitterSearchScraper('from:rogerfederer').get_items()):
  if i > 1000:
    break
  tweets_list.append([tweet.content])


df = pd.DataFrame(tweets_list, columns = ['Tweets'])
print(df.head(10))
df.to_csv('rogerfederer_raw.csv', index = False)

"""## TASK 2

***Read the file from excel sheet***
"""

data = pd.read_csv('rogerfederer_raw.csv')
content = data["Tweets"]

print(content.head())
print("\n")

"""***Clean the newline i.e., "\n" from the tweets***


"""

for i in range (len(content)):
  x = content[i].replace("\n","")
  content[i] = html.unescape(x)

print(content.head(10))
print('\n')

"""***Clean the JSON object data from the tweets***"""

for i in range (len(content)):
  content[i] = re.sub(r"(@[A-Za-z0â€“9_]+)|[^\w\s]|#|http\S+", "", content[i])

print(content.head(10))
print("\n")

for i in range(len(content)):
  content[i] = re.sub("[0-9]", "", content[i])

print(content.head(10))
print("\n")

"""***Change all the words to lowercase***"""

content = [x.lower() for x in content]

"""***Getting the list of the stop-words and removing their punctuation***"""

def remove_stopwords(filename, stopwords):
    x = []
    for i in range(len(filename)):
        for j in stopwords:
            filename[i] = re.sub("\s" + j + "\s", " ", filename[i])

def remove_punctuation(filename):
    x = []
    for i in filename:
        word = re.sub("[~!@#$%^&*()_+,./;'<>?:\"\n]", " ", i) 
        x.append(word)
    return x

stop_words = open('/content/stop_words.txt')
stop_words = remove_punctuation(stop_words)
remove_stopwords(content, stop_words)

df = pd.DataFrame(content)
print(df.head())

print("\n", content)

df.to_csv('rogerfederer_clean.csv', index = False)

"""# TASK 3

***Tokenizing the Strings in the List***
"""

from sklearn.model_selection import train_test_split

content = pd.read_csv("rogerfederer_clean.csv")["0"]
print(content.head())
content_arr = content.to_numpy()

train, test = train_test_split(content_arr, shuffle=True, random_state = 0, train_size=0.8)

print("Shape of train: ", train.shape)
print("Shape of test: ", test.shape)

vocab = []
for i in train:
  word = re.split(" ", str(i)) 
  for j in word:
    if (j not in vocab and j != ""):
      vocab.append(j)

print(len(vocab))
print(vocab)

"""***Creating a Vocabulary out of the Tweets***

***Implementing Bag of Words***
"""

import numpy as np
train_bag_list = []
test_bag_list = []

bow = []
for i in range(len(vocab)):
      bow.append(1)

def bag_of_words(vocab, bow, data_type):
  bag_of_words = []

  for i in data_type:
    # curr_tweet_bag = []
    curr_tweet_bag = bow.copy()

    if (type(i) == str):
      word = re.split(" ", i)    
    else:
      pass

    for j in word:
      if j in vocab:
        idx = vocab.index(j)
        curr_tweet_bag[idx] += 1
    curr_tweet_bag = list(curr_tweet_bag)

    bag_of_words.append(curr_tweet_bag)

  return bag_of_words

train_data = bag_of_words(vocab, bow, train)
test_data = bag_of_words(vocab, bow, test)

"""***For Train Set***"""

for i in range(10):
  print("Tweet No: ", i + 1, ":")
  print(train_data[i])
  print("\n\n")

"""***For Test Set***"""

for i in range(10):
  print("Tweet No: ", i + 1, ":")
  print(test_data[i])
  print("\n\n")

"""# Task 4

***Q1: What data cleaning did you end up doing in Task 2 to create your features in task 3 and why?***

***Ans:*** In order to make sure that as part of the authorship attribution, the tweet is pre-processed enough to have ease in identifying the author, who is Roger Federer in my case, I cleaned data by removing all general words so that the final product may be as specific to Roger Federer as possible. This involved removing JSON objects, links, numericals, punctuations, and stop words. A number of Roger Federer's tweets composed only of emojis and since emojis are unlikely to help pinpoint Roger Federer as the author of a tweet, given that they are often very commonly used by people on twitter, I removed those as well. 

***Q2: In relation to the problem of authorship attribution, which classifiers do you feel would be most appropriate and why? Give a thorough comparison across all the classifiers you have studied in class.***

***Ans:*** The classifiers that we have studied in the classes are namely:

   1. KNN Classifier
   2. Linear Regression Classifier
   3. Logistic Regression Classifier
   4. Support Vector Machine (SVM) Classifier
   5. Neural Network Classifier

Starting off with the KNN classifier, the classifier takes the majority classes of nearest neighbors in consideration to decide which author the tweet would belong to. This would translate to the classifier being very inefficient at the authorship attribution since the problem involves very large distances between the dimensions. Hence, not only would KNN be the most inefficient, it would also be prone to incorrect results owing to the Curse of Dimensionality issue i.e., there are nearest but no near neighbours across dimensions. 

Next, coming onto Linear and Logistic Regression Classifiers, both classifiers assume linearity between the dependent and independent variables. However, there exists no linear relationship in the problem of authorship attributiom hence both classifiers will be of little use in this case.  

SVM, in contrast, scales relatively well to high-dimensional data which is one of the requirements of the authrship attribution problem. Although the SVM may not perform well when the dataset has more noises and may take more time for training large datasets, it offers more generalization in practice and hence is a better classifier than our previous three classifiers i.e., KNN, Linear Regression, and Logistic Regression.

Lastly, we studied Neural Networks. Neural Networks are remarkably efficient and time-saving, designed to keep learning and improving their results continuously. However, htier black-box nature makes it difficult to modufy them and it is tough to track and explain the derivations in their results. Lastly, while efficient, they also require a lot of computational power to run and their results are very tough to explain to the non-technical people. Furthermore, while the Neural Networks are very flexible when it comes to learning, they sadly do require huge amounts of data and do not generalize well to new instances. 

Thus, out of all the classifiers we have studied, SVM and Neural Networks seem to be the top choices. However, out of the data, since Neural Network requires a lot of data and computational power and its results may be tough to explain to the non-technical audience which may be our target audience, it's cons slightly outweigh its pros in this case. Hence, SVMs seem to be the best choice.

***Q3: What is the ambient dimensionality of your solution and how would you determine the
intrinsic dimensionality? Report both in your answer.***

***Ans:*** The ambient dimensionality of my solution is the size of the vocabulary. For the intrinsic dimensionality, we will selectively remove the features and gauge at whether their removal affects the performance of the algorithm. The performance can be looked at by looking at the classification report i.e., Precision, Accuracy, F1 score.
"""